# -*- coding: utf-8 -*-
"""최종 번역 파인튜닝 모델.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fM609YlSUVMe93M8nw10IRXdADwpWVAQ
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive

!pip install datasets

!pip install accelerate

!pip install evaluate

import pandas as pd
import numpy as np
import evaluate
from transformers import (
    BartForConditionalGeneration,
    PreTrainedTokenizerFast,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
)
import re
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast, TrainingArguments, Trainer
import torch
from transformers import BartTokenizer
import seaborn as sns

# 1. 모델 및 토크나이저 불러오기
model_path = './제주도 프로젝트/domain_model'
tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path)
model = BartForConditionalGeneration.from_pretrained(model_path)

"""# 도메인 적응 완료, 제주도 방언 학습 시작"""

main_data = pd.read_csv('./제주도 프로젝트/최종 데이터')
main_data

train_valid_df, test_df = train_test_split(main_data, test_size = 0.2, random_state = 42)

train_df, valid_df = train_test_split(train_valid_df, test_size = 0.25, random_state = 42)

# 분할된 데이터 크기 확인
print("--- 데이터 3분할 결과 ---")
print(f"전체 데이터프레임 크기: {main_data.shape}")
print(f"훈련 데이터프레임 크기: {train_df.shape}")
print(f"검증 데이터프레임 크기: {valid_df.shape}")
print(f"테스트 데이터프레임 크기: {test_df.shape}")

test_df.to_csv('./제주도 프로젝트/real_final_model_test_data', index=False, encoding='utf-8')

# KoBART 토크나이저에 새로운 토큰 추가
new_tokens = ['[제주]', '[표준]']
num_added_tokens = tokenizer.add_tokens(new_tokens)
model.resize_token_embeddings(len(tokenizer))

print(f"추가된 토큰 개수: {num_added_tokens}")
print(f"새로운 토크나이저 어휘 크기: {len(tokenizer)}")

# ⚠️ 수정된 토크나이징 함수
def tokenize_function_with_tags(examples):
    # `input_text`와 `target_text`에 이미 태그가 포함되어 있다고 가정합니다.
    # 토크나이저가 추가된 토큰을 인식하게 됩니다.
    model_inputs = tokenizer(examples['input_text'], max_length=60, truncation=True, padding="max_length")

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(examples['target_text'], max_length=60, truncation=True, padding="max_length")

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Pandas DataFrame을 Hugging Face Dataset 객체로 변환
train_dataset = Dataset.from_pandas(train_df)
valid_dataset = Dataset.from_pandas(valid_df)
test_dataset = Dataset.from_pandas(test_df)

tokenized_train_dataset = train_dataset.map(tokenize_function_with_tags, batched=True, remove_columns=['input_text', 'target_text'])
tokenized_valid_dataset = valid_dataset.map(tokenize_function_with_tags, batched=True, remove_columns=['input_text', 'target_text'])
tokenized_test_dataset = test_dataset.map(tokenize_function_with_tags, batched=True, remove_columns=['input_text', 'target_text'])

!pip install sacrebleu

!pip install bert_score

bleu_metric = evaluate.load("sacrebleu")
chrf_metric = evaluate.load("chrf")
bertscore_metric = evaluate.load("bertscore")

def compute_metrics(eval_preds):
    preds, labels = eval_preds
    if isinstance(preds, tuple):
        preds = preds[0]

    # 모델이 생성한 예측 토큰을 문자열로 디코딩
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    # -100 패딩 값 처리 후 정답 토큰을 문자열로 디코딩
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # ✅ 평가를 위해 예측과 정답 문장을 정규화합니다.
    decoded_preds = [re.sub(r'\s+', ' ', s).strip() for s in decoded_preds]
    decoded_labels = [re.sub(r'\s+', ' ', s).strip() for s in decoded_labels]

    # SacreBLEU 계산을 위해 참조 문장을 리스트의 리스트 형태로 변환
    decoded_labels_list = [[label] for label in decoded_labels]

    # SacreBLEU 점수 계산
    bleu_result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels_list)

    # ✅ CHRF 점수 계산
    chrf_result = chrf_metric.compute(predictions=decoded_preds, references=decoded_labels_list)

    # ✅ BERTScore 점수 계산 (매우 느릴 수 있음)
    # lang='ko'를 지정하여 한국어에 최적화된 BERT 모델을 사용합니다.
    bertscore_result = bertscore_metric.compute(
        predictions=decoded_preds,
        references=decoded_labels,
        lang='ko'
    )

    # 모든 점수를 딕셔너리에 담아 반환합니다.
    result = {
        "sacrebleu": bleu_result["score"],
        "chrf": chrf_result["score"],
        "bertscore": np.mean(bertscore_result["f1"]) # F1 점수의 평균을 사용합니다.
    }

    return result

from transformers import EarlyStoppingCallback

# 학습 설정을 정의
training_args = Seq2SeqTrainingArguments(
    output_dir="./kobart_jeju_translation",
    eval_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=128,
    per_device_eval_batch_size=128,
    num_train_epochs=5,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=500,
    save_strategy="epoch",
    report_to='none',
    fp16=True, # ✅ 학습 시간 단축을 위한 혼합 정밀도 학습
    gradient_accumulation_steps=16, # ✅ 그래디언트 누적 단계 설정 (배치 사이즈를 2배로 사용하는 효과)
    predict_with_generate=True,
    generation_num_beams=4,
    generation_max_length=128,
)

# Seq2SeqTrainer 객체 정의
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_valid_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

print("\n--- 모델 학습 시작 (trainer) ---")
result = trainer.train()
print("\n--- 학습 완료 ---")

# --- 학습된 모델 저장 ---
output_path = "./제주도 프로젝트/real_final_model2"
trainer.save_model(output_path)









